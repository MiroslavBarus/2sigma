{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-02T18:16:38.157874Z",
     "iopub.status.busy": "2020-06-02T18:16:38.157277Z",
     "iopub.status.idle": "2020-06-02T18:16:38.163296Z",
     "shell.execute_reply": "2020-06-02T18:16:38.162818Z",
     "shell.execute_reply.started": "2020-06-02T18:16:38.157800Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from modeling import feature_engineering, closest, agg_function,agg_diff_fun, save_object,load_object,get_best_params\n",
    "import numpy as np\n",
    "import reverse_geocoder as revgc\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-02T18:16:39.005175Z",
     "iopub.status.busy": "2020-06-02T18:16:39.004566Z",
     "iopub.status.idle": "2020-06-02T18:17:19.105774Z",
     "shell.execute_reply": "2020-06-02T18:17:19.105086Z",
     "shell.execute_reply.started": "2020-06-02T18:16:39.005098Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train= pd.read_json('train.json')\n",
    "df_test= pd.read_json('test.json')\n",
    "metro= pd.read_csv('MetroStanice.csv')\n",
    "df_train[\"training\"]=1\n",
    "df_test[\"training\"]=0\n",
    "\n",
    "df= pd.concat([df_train,df_test],sort=True).reset_index(drop=True)\n",
    "\n",
    "mapping_features= pd.read_csv('feature_deduplication.csv')\n",
    "mydict= dict(zip(mapping_features.original_feature, mapping_features.unique_feature))\n",
    "#features and photos\n",
    "df[\"len_features\"] = df[\"features\"].apply(len)\n",
    "df[\"len_photos\"] = df[\"photos\"].apply(len)\n",
    "df['features']= df['features'].apply(lambda x: [item.lower() for item in x]) \n",
    "df['features_txt']= df['features'].apply(lambda x: ' '.join(x))\n",
    "df['features']=df['features'].apply(lambda x: [mydict[item] if item in mydict else item for item in x ])\n",
    "#pocet features\n",
    "#\n",
    "\n",
    "#timefromepoch\n",
    "import datetime\n",
    "#df['tstamp'] =df['created'].apply(lambda x:  datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S').timestamp())\n",
    "#Datumove\n",
    "#df[\"date\"]=  df['created'].str[:7]\n",
    "df['created']=pd.to_datetime(df[\"created\"])\n",
    "#df['day'] = df['created'].dt.day\n",
    "#df['days']=(df[\"created\"]-min(df[\"created\"])).apply(lambda x: x.days)\n",
    "df['dayofweek'] = df['created'].dt.dayofweek\n",
    "#cylicke\n",
    "df['hr'] = df['created'].dt.hour\n",
    "df['mnth'] = df['created'].dt.month\n",
    "df['nwords_description']= df['description'].apply(lambda x: x.count(' ')+1)\n",
    "df['price_digits']=df['price'].apply(lambda x: len(str(x)))\n",
    "df['price'] = np.log(df['price'])\n",
    "\n",
    "# GEO\n",
    "coordinates= df[[\"latitude\",\"longitude\"]].values\n",
    "coordinates_tuple=tuple((x[0],x[1]) for x in coordinates)\n",
    "adresy=revgc.search(coordinates_tuple)\n",
    "df=pd.concat([df,pd.DataFrame(adresy)],axis=1)\n",
    "\n",
    "\n",
    "#label encoding\n",
    "event_dictionary ={'low' : 1, 'medium' : 2, 'high' : 3} \n",
    "df['interest_integer'] =df['interest_level'].map(event_dictionary) \n",
    "\n",
    "#interakcie zakladna intuicia\n",
    "df['bed+bath']=df['bedrooms']+df['bathrooms']\n",
    "df['price/bed']=df['price']/(df['bedrooms']+1e-16)\n",
    "df['price/bed+bath']=df['price']/(df['bed+bath']+1e-16)\n",
    "\n",
    "training='training'\n",
    "label='interest_integer'\n",
    "\n",
    "metro=metro.loc[:,['lat','lon']]\n",
    "metro=metro.to_dict('records')\n",
    "dfmetro=df.loc[:,['latitude','longitude']]\n",
    "dfmetro=dfmetro.to_dict('records')\n",
    "x=[]\n",
    "for i in dfmetro:\n",
    "    x.append(closest(metro, i))\n",
    "df['metro']=x\n",
    "\n",
    "\n",
    "#drop columns\n",
    "df=df.drop(columns=['photos','created','listing_id','lat','lon','latitude','longitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-28T18:55:28.205544Z",
     "iopub.status.busy": "2020-05-28T18:55:28.205338Z",
     "iopub.status.idle": "2020-05-28T20:50:37.532309Z",
     "shell.execute_reply": "2020-05-28T20:50:37.531873Z",
     "shell.execute_reply.started": "2020-05-28T18:55:28.205523Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-28 20:55:28.208360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:52<00:00,  3.78s/it]\n",
      "100%|██████████| 13/13 [00:48<00:00,  3.74s/it]\n",
      "100%|██████████| 12/12 [00:44<00:00,  3.73s/it]\n",
      "100%|██████████| 11/11 [00:43<00:00,  3.92s/it]\n",
      "100%|██████████| 10/10 [00:37<00:00,  3.73s/it]\n",
      "100%|██████████| 9/9 [00:29<00:00,  3.27s/it]\n",
      "100%|██████████| 8/8 [00:25<00:00,  3.21s/it]\n",
      "100%|██████████| 7/7 [00:22<00:00,  3.18s/it]\n",
      "100%|██████████| 6/6 [00:21<00:00,  3.55s/it]\n",
      "100%|██████████| 5/5 [00:17<00:00,  3.51s/it]\n",
      "100%|██████████| 4/4 [00:12<00:00,  3.12s/it]\n",
      "100%|██████████| 3/3 [00:09<00:00,  3.07s/it]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after numeric feature selection:  ['price', 'len_photos', 'hr', 'price/bed'] 0.6776139370308577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:43<00:00,  3.27s/it]\n",
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after TOP N features:  ['price', 'len_photos', 'hr', 'price/bed', 'hardwood', 'no fee'] 0.6758182856319486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [09:00<00:00, 77.20s/it]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after numeric features aggregated by categorical features ['price', 'len_photos', 'hr', 'price/bed', 'hardwood', 'no fee', 'mean_bathrooms_by_building_id', 'median_bathrooms_by_building_id', 'sum_bathrooms_by_building_id', 'std_bathrooms_by_building_id', 'bathrooms-mean_bathrooms_by_building_id', 'bathrooms-median_bathrooms_by_building_id', 'mean_price_by_building_id', 'median_price_by_building_id', 'sum_price_by_building_id', 'std_price_by_building_id', 'price-mean_price_by_building_id', 'price-median_price_by_building_id'] 0.6624599534991646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      " 17%|█▋        | 1/6 [00:05<00:29,  5.96s/it]\u001b[A\n",
      " 33%|███▎      | 2/6 [00:19<00:38,  9.69s/it]\u001b[A\n",
      " 25%|██▌       | 1/4 [01:19<03:58, 79.62s/it]\n",
      "  0%|          | 0/6 [00:06<?, ?it/s]\u001b[A\n",
      " 50%|█████     | 2/4 [02:26<02:31, 75.94s/it]\n",
      "  0%|          | 0/6 [00:06<?, ?it/s]\u001b[A\n",
      " 75%|███████▌  | 3/4 [03:32<01:12, 72.76s/it]\n",
      "  0%|          | 0/6 [00:06<?, ?it/s]\u001b[A\n",
      "100%|██████████| 4/4 [04:38<00:00, 69.58s/it]\n",
      "  0%|          | 0/38 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numeric features interactions ['price', 'len_photos', 'hr', 'price/bed', 'hardwood', 'no fee', 'mean_bathrooms_by_building_id', 'median_bathrooms_by_building_id', 'sum_bathrooms_by_building_id', 'std_bathrooms_by_building_id', 'bathrooms-mean_bathrooms_by_building_id', 'bathrooms-median_bathrooms_by_building_id', 'mean_price_by_building_id', 'median_price_by_building_id', 'sum_price_by_building_id', 'std_price_by_building_id', 'price-mean_price_by_building_id', 'price-median_price_by_building_id', 'price+price_digits', 'bathrooms+price', 'bed+bath+price/bed', 'bathrooms+price/bed+bath', 'bedrooms+price/bed', 'bathrooms+price/bed', 'price+price/bed+bath', 'bed+bath+price/bed+bath', 'price_digits+price/bed', 'bathrooms+len_photos', 'price+len_photos', 'price/bed+metro', 'len_photos+price_digits', 'price/bed+price/bed+bath', 'len_features+nwords_description', 'nwords_description+price/bed', 'len_photos+metro', 'len_features+price/bed', 'bathrooms+nwords_description', 'len_photos+nwords_description'] 0.6544216138226803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [04:20<00:00,  6.85s/it]\n",
      "100%|██████████| 37/37 [04:12<00:00,  6.82s/it]\n",
      "100%|██████████| 36/36 [03:43<00:00,  6.20s/it]\n",
      "100%|██████████| 35/35 [03:37<00:00,  6.20s/it]\n",
      "100%|██████████| 34/34 [03:35<00:00,  6.35s/it]\n",
      "100%|██████████| 33/33 [03:29<00:00,  6.36s/it]\n",
      "100%|██████████| 32/32 [03:25<00:00,  6.41s/it]\n",
      "100%|██████████| 31/31 [03:15<00:00,  6.32s/it]\n",
      "100%|██████████| 30/30 [03:11<00:00,  6.39s/it]\n",
      "100%|██████████| 29/29 [03:04<00:00,  6.35s/it]\n",
      "100%|██████████| 28/28 [02:57<00:00,  6.35s/it]\n",
      "100%|██████████| 27/27 [02:49<00:00,  6.28s/it]\n",
      "100%|██████████| 26/26 [02:43<00:00,  6.27s/it]\n",
      "100%|██████████| 25/25 [02:18<00:00,  5.55s/it]\n",
      "100%|██████████| 24/24 [02:20<00:00,  5.84s/it]\n",
      "100%|██████████| 23/23 [02:07<00:00,  5.55s/it]\n",
      "100%|██████████| 22/22 [02:07<00:00,  5.80s/it]\n",
      "100%|██████████| 21/21 [02:01<00:00,  5.81s/it]\n",
      "100%|██████████| 20/20 [01:54<00:00,  5.72s/it]\n",
      "100%|██████████| 19/19 [01:40<00:00,  5.28s/it]\n",
      "100%|██████████| 18/18 [01:36<00:00,  5.37s/it]\n",
      "100%|██████████| 17/17 [01:29<00:00,  5.27s/it]\n",
      "100%|██████████| 16/16 [01:18<00:00,  4.88s/it]\n",
      "100%|██████████| 15/15 [01:07<00:00,  4.50s/it]\n",
      "100%|██████████| 14/14 [01:05<00:00,  4.66s/it]\n",
      "100%|██████████| 13/13 [00:55<00:00,  4.29s/it]\n",
      "100%|██████████| 12/12 [00:51<00:00,  4.26s/it]\n",
      "100%|██████████| 11/11 [00:45<00:00,  4.13s/it]\n",
      "100%|██████████| 10/10 [00:44<00:00,  4.43s/it]\n",
      "100%|██████████| 9/9 [00:34<00:00,  3.88s/it]\n",
      "100%|██████████| 8/8 [00:29<00:00,  3.64s/it]\n",
      "100%|██████████| 7/7 [00:22<00:00,  3.23s/it]\n",
      "100%|██████████| 6/6 [00:20<00:00,  3.46s/it]\n",
      "100%|██████████| 5/5 [00:17<00:00,  3.42s/it]\n",
      "100%|██████████| 4/4 [00:13<00:00,  3.40s/it]\n",
      "100%|██████████| 3/3 [00:09<00:00,  3.21s/it]\n",
      "100%|██████████| 2/2 [00:06<00:00,  3.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numeric features interactions_RVE ['price', 'hr', 'no fee', 'sum_bathrooms_by_building_id', 'std_price_by_building_id', 'price+price_digits', 'bathrooms+price/bed+bath', 'price+price/bed+bath', 'len_photos+price_digits'] 0.6505839752162139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building_id CB_target_encoding 0.6500204405139437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1/7 [00:22<02:15, 22.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id CB_target_encoding 0.6197276719259592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 3/7 [00:52<01:21, 20.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id LOO_target_encoding 0.6178886874295566\n",
      "name count_encoder 0.6159339788809312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [02:54<00:00, 24.87s/it]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]/home/martas/anaconda3/envs/mirko/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after non-numeric features encoding:  ['price', 'hr', 'no fee', 'sum_bathrooms_by_building_id', 'std_price_by_building_id', 'price+price_digits', 'bathrooms+price/bed+bath', 'price+price/bed+bath', 'len_photos+price_digits', 'building_id', 'manager_id', 'name'] 0.6159339788809312\n",
      "1 <function stemming_tokenizer at 0x7fb0cf5c55f0> 1\n",
      "0.6163117895906415\n",
      "1 <function stemming_tokenizer at 0x7fb0cf5c55f0> 2\n",
      "1 <function stemming_tokenizer at 0x7fb0cf5c55f0> 3\n",
      "1 <function stemming_tokenizer at 0x7fb0cf5c55f0> 4\n",
      "1 <function stemming_tokenizer at 0x7fb0cf5c55f0> 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martas/anaconda3/envs/mirko/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 <modeling.LemmaTokenizer object at 0x7fb0aeb99a50> 1\n",
      "1 <modeling.LemmaTokenizer object at 0x7fb0aeb99a50> 2\n",
      "1 <modeling.LemmaTokenizer object at 0x7fb0aeb99a50> 3\n",
      "1 <modeling.LemmaTokenizer object at 0x7fb0aeb99a50> 4\n",
      "1 <modeling.LemmaTokenizer object at 0x7fb0aeb99a50> 5\n",
      "1 None 1\n",
      "0.6162974341818417\n",
      "1 None 2\n",
      "1 None 3\n",
      "1 None 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [05:34<11:08, 334.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 None 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martas/anaconda3/envs/mirko/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 <function stemming_tokenizer at 0x7fb0cf5c55f0> 1\n",
      "2 <function stemming_tokenizer at 0x7fb0cf5c55f0> 2\n",
      "2 <function stemming_tokenizer at 0x7fb0cf5c55f0> 3\n",
      "2 <function stemming_tokenizer at 0x7fb0cf5c55f0> 4\n",
      "2 <function stemming_tokenizer at 0x7fb0cf5c55f0> 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martas/anaconda3/envs/mirko/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 <modeling.LemmaTokenizer object at 0x7fb0aeb99a50> 1\n",
      "2 <modeling.LemmaTokenizer object at 0x7fb0aeb99a50> 2\n",
      "2 <modeling.LemmaTokenizer object at 0x7fb0aeb99a50> 3\n",
      "2 <modeling.LemmaTokenizer object at 0x7fb0aeb99a50> 4\n",
      "2 <modeling.LemmaTokenizer object at 0x7fb0aeb99a50> 5\n",
      "2 None 1\n",
      "2 None 2\n",
      "2 None 3\n",
      "2 None 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [11:33<05:41, 341.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 None 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martas/anaconda3/envs/mirko/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 <function stemming_tokenizer at 0x7fb0cf5c55f0> 1\n",
      "3 <function stemming_tokenizer at 0x7fb0cf5c55f0> 2\n",
      "3 <function stemming_tokenizer at 0x7fb0cf5c55f0> 3\n",
      "3 <function stemming_tokenizer at 0x7fb0cf5c55f0> 4\n",
      "3 <function stemming_tokenizer at 0x7fb0cf5c55f0> 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martas/anaconda3/envs/mirko/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 <modeling.LemmaTokenizer object at 0x7fb0aeb99a50> 1\n",
      "3 <modeling.LemmaTokenizer object at 0x7fb0aeb99a50> 2\n",
      "3 <modeling.LemmaTokenizer object at 0x7fb0aeb99a50> 3\n",
      "3 <modeling.LemmaTokenizer object at 0x7fb0aeb99a50> 4\n",
      "3 <modeling.LemmaTokenizer object at 0x7fb0aeb99a50> 5\n",
      "3 None 1\n",
      "3 None 2\n",
      "3 None 3\n",
      "3 None 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [18:05<00:00, 361.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 None 5\n",
      "after text feature tokenization plus PCA:  ['price', 'hr', 'no fee', 'sum_bathrooms_by_building_id', 'std_price_by_building_id', 'price+price_digits', 'bathrooms+price/bed+bath', 'price+price/bed+bath', 'len_photos+price_digits', 'building_id', 'manager_id', 'name'] 0.6159339788809312\n",
      "2020-05-28 22:50:36.823854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "agg_dict={agg_function: ['mean','median','sum','std'],agg_diff_fun: ['mean', 'median']}\n",
    "training='training'\n",
    "label='interest_integer'\n",
    "pca_list=[1,2,3,4,5]\n",
    "num_features=50\n",
    "clf = RandomForestClassifier(max_depth=5,criterion='gini', random_state=0,n_estimators=150,n_jobs=-1)\n",
    "df_out,columns,good_encoders,dp_table,df_tokenizer=feature_engineering(df,training,label,num_features,agg_dict,pca_list,clf =clf)\n",
    "save_object([df_out,columns,good_encoders,dp_table,df_tokenizer],clf.__class__.__name__+'_thesis_v_rve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2020-06-02T08:39:55.761520Z",
     "iopub.status.idle": "2020-06-02T08:39:55.761793Z"
    }
   },
   "outputs": [],
   "source": [
    "agg_dict={agg_function: ['mean','median','sum','std'],agg_diff_fun: ['mean', 'median']}\n",
    "training='training'\n",
    "label='interest_integer'\n",
    "pca_list=[1,2,3,4,5]\n",
    "num_features=50\n",
    "clf = RandomForestClassifier(max_depth=5,criterion='gini', random_state=0,n_estimators=150,n_jobs=-1)\n",
    "df_out,columns,good_encoders,dp_table,df_tokenizer=feature_engineering(df,training,label,num_features,agg_dict,pca_list,clf =clf)\n",
    "save_object([df_out,columns,good_encoders,dp_table,df_tokenizer],clf.__class__.__name__+'_thesis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-02T18:17:24.074309Z",
     "iopub.status.busy": "2020-06-02T18:17:24.073713Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-02 20:17:24.079344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [10:27<00:00, 44.85s/it]\n",
      "100%|██████████| 13/13 [09:20<00:00, 43.10s/it]\n",
      "100%|██████████| 12/12 [09:00<00:00, 45.06s/it]\n",
      "100%|██████████| 11/11 [08:26<00:00, 46.01s/it]\n",
      "100%|██████████| 10/10 [08:11<00:00, 49.16s/it]\n",
      "100%|██████████| 9/9 [05:54<00:00, 39.34s/it]\n",
      "100%|██████████| 8/8 [05:33<00:00, 41.63s/it]\n",
      "100%|██████████| 7/7 [05:16<00:00, 45.28s/it]\n",
      "100%|██████████| 6/6 [04:17<00:00, 42.88s/it]\n",
      "100%|██████████| 5/5 [02:48<00:00, 33.63s/it]\n",
      "100%|██████████| 4/4 [01:37<00:00, 24.26s/it]\n",
      "100%|██████████| 3/3 [01:09<00:00, 23.12s/it]\n",
      "100%|██████████| 2/2 [00:39<00:00, 19.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after numeric feature selection:  ['bathrooms', 'price', 'len_features', 'len_photos', 'hr', 'nwords_description', 'price/bed+bath', 'metro'] 0.6295682664973432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [35:08<00:00, 42.18s/it]\n",
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after TOP N features:  ['bathrooms', 'price', 'len_features', 'len_photos', 'hr', 'nwords_description', 'price/bed+bath', 'metro', 'laundry', 'elevator', 'hardwood', 'cats allowed', 'concierge', 'no fee', 'fitness center', 'pre war', 'exclusive', 'simplex', 'reduced fee', 'furnished'] 0.6124358184149072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 3/7 [1:34:39<1:57:39, 1764.83s/it]"
     ]
    }
   ],
   "source": [
    "parameters=pd.read_csv('RandomForestClassifier_grid_results_extra.csv')\n",
    "best_params=get_best_params(parameters,'score')\n",
    "best_params\n",
    "agg_dict={agg_function: ['mean','median','sum','std'],agg_diff_fun: ['mean', 'median']}\n",
    "training='training'\n",
    "label='interest_integer'\n",
    "pca_list=[1,2,3,4,5]\n",
    "num_features=50\n",
    "clf = RandomForestClassifier( random_state=0,n_jobs=-1,**best_params)\n",
    "df_out,columns,good_encoders,dp_table,df_tokenizer=feature_engineering(df,training,label,num_features,agg_dict,pca_list,clf =clf)\n",
    "save_object([df_out,columns,good_encoders,dp_table,df_tokenizer],clf.__class__.__name__+'_thesis_best_grid_params_extra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
